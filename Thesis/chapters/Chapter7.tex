This thesis has examined theoretical aspects of modelling topographic maps and applied this theory to analysing biological data arising from experiments on the mouse retinocollicular system. The key findings are:
\begin{enumerate}
	\item A rigorous analysis of neural activity and STDP as a mechanism for topographic refinement was performed and applied to mouse retinotopic data. This required the development of a framework which was able to explain existing retinotopic data and regression on this data made the prediction of a plasticity time-scale being 2.5 seconds in topographic development.
	\item A stochastic development hypothesis was interrogated in the context of optical imaging data and a best-in-class retinotopic development model. The modelling approached showed that stochasticity does not need to be invoked to explain existing data.
	\item An extension of the unified model was proposed which allowed for more efficient computation and therefore an opportunity to further develop an understanding of the interaction of model mechanisms as well allowing modern regression techniques to be performed. The new framework offered a 100-fold speed up correctly predicted the developmental time-course differences between wild-type and $\beta2^{-/-}$ mice.
	\item Modern understandings of the role of competition were abstracted into the Elastic Net, a heuristic solver for NP-complete problems such as the Travelling Salesman Problem (TSP), and explored the consequences of this abstraction both for solving the TSP and in the formation of feature selective maps. The new heuristic was found to be competitive with state of the art solvers and provided improved predictions of the underlying retinotopy of feature maps.
\end{enumerate}

\section{Neural Field Theory}
The first study was a theoretical examination into the role of activity in the refinement of the topographic projection using a neural field theory model of activity coupled with a spike-timing-dependent plasticity rule \cite{Robinson2011-ve}. The theory lays the groundwork for how the refinement of the topographic projection operates and provides a framework in which all spatio-temporal correlations can be incorporated. This framework is valuable because existing models assume that there are no temporal effects and incorporate activity by isotropic spatial correlations under which some version of the Hebb rule is presumed to operate: the implicit averaging of temporal statistics makes the models inadequate for addressing time sensitive developmental questions and the assumed spatial isotropy limits predictions about anisotropic functional behaviours. The neural field theory model was then fitted via MCMC regression to existing data and was found to provide a good quantitative account of both wild-type and $\beta2^{-/-}$ mouse maps. This regression also made a quantitative prediction about the time-scale on which plasticity operates which is on the order of seconds. This prediction is substantially longer than plasticity measurements observed in other systems but is similar to the time-scale of the average spontaneous wave of activity in the developing mouse. It seems likely that the longer plasticity window allows all information in the wave to be correctly integrated \cite{Xu2015-uc}.

\paragraph{Limitations} The study was limited in several key ways. First, it makes several simplifying assumptions about the presentation and duration of the wave-activity. These assumptions allow an analytical solution to be derived in the asymptotic cases which provides a good understanding of the principles but limits the realism. Second, the regression analysis is feasible because the analytic solution is relatively cheap to compute: a numerical solution integrated over the entire time course is prohibitive in the context of Bayesian regression because many thousands of samples need to be drawn for good convergence. Third, the study was one-dimensional which naturally places restrictions on data interpretation. Fourth, recurrent connections were estimated using electrophysiological recordings and assumed to be static throughout the developmental process \cite{Phongphanphanee2014-in}. Finally, there were several parameters which were identified to not have any effect on the relevant statistic of organisation width, but which may affect the biological plausibility of the model: these parameters and their consequences on other measurable aspects of topographic development should be carefully explored. These limitations offer some natural research pathways for future work.

\paragraph{Future Work} The most straightforward extension of the study is to expand the framework into two dimensions which would have the advantage of being a natural representation of the sub-cortical sheet. While analytical solutions may exist for this dynamical system in two dimensions they would necessitate the same simplifying assumptions and it seems more fruitful to implement the spontaneous input activity directly either by estimating from data directly or using the generative model developed by Godfrey et. al. (2009) \cite{Ackman2012-uu, Stafford2009, Godfrey2009-rs, Eglen2014-fo}. Taking a numerical approach would immediately introduce the problem of computational cost but this can be mitigated by realising that the largest computational cost is a matrix multiplication operation which can be highly parallelised using the GPU architectures employed in Chapters \ref{chapter:distributed} and \ref{chapter:elastic}. This can be augmented with efficient numerical solvers which have been developed for the Julia computing language \cite{Sequeira2022}.

A more theoretical extension would be to examine the effect of allowing dynamic recurrent connections. There have been no studies on SC lateral connectivity patterns being rewired during these developmental stages and exploring this assumption in a theoretical context would offer falsifiable predictions which can be rigorously tested biologically. This direction of research would necessitate a more speculative approach in the initial stages but could yield phenomenological insights, particularly when combined with anisotropic inputs. These inputs, in addition with an evolving input kernel would be a natural extension of earlier theoretical examinations into recurrent connectivity \cite{Sirosh1994-zv}.

A final line of immediate future research would be examining the model as a candidate for feature selectivity; this was partially explored in Chapter \ref{chapter:elastic} with the Elastic Net. The retinotopic map to SC in mice has traditionally thought to not encode feature maps but recent studies have shown preferences for orientation \cite{Ahmadlou2015-jw}. A model has been proposed to explain these orientation preferences in terms of retinal wave directional biases and ON-OFF timing delays \cite{Teh2022-tp}. 

\section{Analysis of the Stochastic Development Hypothesis}
The second study involved an examination of a stochastic development hypothesis where it was postulated that neural developmental mechanisms are stable but ultimately stochastic in nature evidenced by a dataset of EphA3 heterozygotes which present functionally as wild-type, homozygotes, and some mixed state in between \cite{Owens2015-zv}. Following a thorough reanalysis of the dataset by David Willshaw a computational pipeline was developed which allowed the generation of lattice objects from a functional scan of the output of an anatomical model. The anatomical model was the Tsigankov-Koulakov model as it offers the most parsimonious explanation of EphA3 knock-in mice while incorporating chemotactic and activity based interactions \cite{Triplett2011-jk, Hjorth2015-le}. An extensive set of simulations revealed through both lattice plot reconstructions and anatomical flood injections that the EphA3 projection develops smoothly and in a stereotypical fashion. Notably, it was observed that the EphA3 projection is restricted to caudal regions of the colliculus but the wild-type cells tend to make connections across the entire colliculus becoming completely segregated only for values of EphA3 knock-in higher than what is typically observed in both homozygotes and heterozygotes; this challenges the notion that these two populations are segregated in the EphA3 homozygotes. 

The lattice method allows for algorithmic generation of quantitative summary statistics and the dataset was used to begin to explore the parameter space of the model and provide quantitative links between observed data and model predictions; understanding the statistical properties of generative statistical models is important to avoid erroneous conclusions and it offers a method to constrain model parameters by data which enhances the explanatory and exploratory power of the model. This exploration of parameters and statistical hypotheses was  performed by generating hundred sample batches for a parameter sweep of the parameter controlling the magnitude of EphA3 knock-in. The statistics relevant to the question of mixed maps were examined: the visual field overlap, part-map qualities, and the mean anatomical projection. These lend support to the notion of a smoothly deforming effect induced by the EphA3 knock-in while validating the likelihood that heterozygotes and homozygotes come from similar distributions despite there being significant variance.

A pilot study was performed to modelling the interacting effects of neural activity and chemotaxis whereby the activity scaling was up-regulated and down-regulated by up to an order of magnitude of the default parameter. In general the interactions are reasonably complex: at high activity scaling there is a complete dominance of the activity mechanism and no double projections are present, with a moderate increase of activity the collapse point of the EphA3 projection is less defined but the receptive fields are tighter, a moderate decrease of activity yields a more defined collapse point but wider receptive fields, and very low activity scaling there is a regular projection from wild-type cells but the EphA3 projection has several reversals of polarity and appears to be generally unstructured. The effect of the $\beta2^{-/-}$ was also examined by down regulating the activity scaling and also widening the correlation window. This was most similar to the low activity scaling with wide receptive fields and a well defined collapse point. This is expected from existing biological theory that the $\beta2$ knock-out mice have broader receptive fields and less refined projection but is discordant with the data presented by Owens et. al. (2015) \cite{Owens2015-zv}.

\paragraph{Limitations} 
There are two key theoretical limitations in this study: computational cost and an inconclusive understanding of the interaction between mechanisms. The computational demands of the model at a resolution comparable to the data are substantial: it currently takes up to a full day for a single instance of the model with a set of given parameters to converge. This was prohibitive in generating samples and necessitated a reduction of cellular resolution by a factor of five. Even with this reduction only 100 samples were generated per parameter instance which, while informative, is not optimal. An MCMC regression would be infeasible with this model implementation even when substantial parallelisation is employed. The large computational cost also limits thorough exploration of mechanism interaction. Several non-intuitive features were present in a basic pilot study of activity manipulation and given the model can act as a universal function approximator it is crucial that the interaction between mechanisms is understood before they are manipulated.

A key biological limitation is the inability to reproduce the $\beta2^{-/-}$ observations made by Owens et. al. (2015). This could be due to the modelling process: there could be a fundamental component of the model that cannot accurately account for activity and chemotactic interactions. It could also be a calibration issue: the model has not been extensively calibrated to understand this phenotype and the scaling parameters might occupy an incorrect region of parameter space. Finally, it could be a limitation of the data: the $\beta2^{-/-}$ datasets were not made available and the sample size was small ($N=2$). 
\paragraph{Future Work}
A straightforward extension of this study would be learning the parameters appropriate for the model and optical imaging datasets offer an excellent candidate training set. The computational limitations imposed by the model are the most challenging component of this direction and there are two possible solutions: define a new model which offers the same explanatory power or update the computational processes in the existing model. Updating the model is slightly more challenging: the Metropolis-Hastings procedure cannot be parallelised and will therefore always impose a linear cost of calling the energy function in terms of the number of nodes. Therefore, improvements must come from the evaluation of the energy function itself. The most computationally intensive component of the energy function is the neural activity term which involves a matrix calculation composed of all possible synaptic pairings $N^2$. This can be accelerated via GPU parallelisation a concept introduced and explored in Chapters \ref{chapter:distributed} and \ref{chapter:elastic}. However, this requires construction at each time step of a $10^8$ matrix which puts a severe memory constraint on the GPU would require optimisation. With these considerations in mind  the first option of defining a new model was explored in Chapter \ref{chapter:distributed} and will be discussed in the following section. 

The lattice method allows easy computation of statistics that offer an ideal target on which to train models on. Currently, the methodology is complicated and difficult for end-users to interface with: the code base is unwieldy and there are several hyper-parameters which require expert knowledge to properly utilise. These complications are largely technical and careful analysis of the algorithm would allow these technicalities to be abstracted into a code engine. Further to this, much of the lattice method pipeline can be automated from data input. A useful line of future research will be to carefully analyse the program and construct a package which automates the construction of a lattice from an optical scan: ideally there would be no need to identify the colliculus or specifying part-map regions by hand. A user-friendly package would greatly assist experimentalists and theoreticians alike and result in high quality whole-map analysis of optical image datasets and provided statistical test-beds for theoretical questions.
\section{Distributed Kernels Model}
The third study involved an extension to the Tsigankov-Koulakov model to solve the problem of high computational demands which limit the ability to perform detailed analysis of parameter spaces and perform model training. The model was redefined by combining several principles discussed in the literature review; see Section \ref{sec:models}. The model was formulated to model the likely probability distribution of contacts for each retinal afferent. Each of these distributions was approximated via a linear combination Gaussian kernels which were sampled from in the final step and helped to keep computational and memory demands low. The energy formulation from Tsigankov-Koulakov model was maintained as well as the Type I graded matching mechanism in the gradient. The minimisation procedure was gradient descent based and behaved in a fashion similar to the Goodhill-Simpson model. These combined modifications resulted in a model which had a 100 fold speed-up against the Tsigankov-Koulakov model. 

The model was then used to examine several genotypes previously discussed in two key ways: tracer injections colour coded to their retinal provenance and lattice objects generated on the basis of the anatomical structure predicted in the model. These studies showed that the model was capable of reproducing the phenotypical presentation of each of the mutants with parameters changes appropriate to the genotype. A key component in these studies was the fixing of parameters that did not directly contribute to the mechanisms manipulated in the genotype. The model accurately reproduced the phenomenological components of each genotype and provided a comparable lattice representation. 

Finally, a pilot study was performed examining the developmental time course predicted by the model. The model uses a gradient descent methodology which is local and evolves from a set of initial conditions. The steps in the scheme present a plausible evolution series for map development and can therefore be used to time related questions as well as making predictions about critical points in development. While preliminary the study showed that the model can capture the developmental time delay observed in the $\beta2^{-/-}$ knock-out mouse without significant changes to other parameters as was required in other unified modelling frameworks \cite{Lyngholm2019-fs}. 
\paragraph{Limitations}
There was no new biological insight generated by this study --- every genotype examined has been previously examined and adequately explained by, in many cases several, existing modelling methodologies. This is largely expected because the model was fashioned as a computationally efficient extension of the existing modelling methodologies; given the plurality of models and general consensus on mechanisms it would be ineffective to craft an entirely new model. However, computational efficiency is useful only when it can be used to generate new scientific insights and therefore a major challenge this framework faces is the prediction of novel and falsifiable experimental hypotheses.
\paragraph{Future Work}
As the model is a conceptual extension of existing methodologies novel insight is likely to be generated resolving challenges which current unified models find difficult. The most pressing challenge is the assignment of model parameters and relative interaction between mechanisms. The framework outlined in Chapter \ref{chapter:lattice} provides a pipeline for these types of questions and various optical imaging datasets in conjunction with the lattice method provide an avenue for regression and Bayesian parameter analysis which would have otherwise been infeasible. This would provide an indication of the utility of model and is a necessary step before any future research is undertaken.

An exciting secondary research vector which can be pursued is the exploration of modelling the entire time-course of development. This research has been approached with the Tsigankov-Koulakov model \cite{Lyngholm2019-fs}. The approach is necessarily limited because the minimisation procedure is a mathematical convenience for minimisation rather than a routine which mimics underlying fundamental biology; the creation and deletion of very long range synapses presents the physical challenge of afferent crossover and is biologically unrealistic. The pilot study demonstrated promising results modelling the $\beta2^{-/-}$ mutant but will require more rigorous and principled analysis. The model may be examined theoretically by analysing the precise relationship the differential operator has with real biological time. Then, a data-driven analysis can be performed to assess model efficacy and this can be used as the basis to make falsifiable predictions.

 \section{Elastic Neighbourhood}
The final study involved abstracting modern understandings of the role of competition in retinotopic development and incorporating them into the NP-complete Travelling Salesman algorithm: the Elastic Net. The original Elastic Net model was itself an abstraction of the Tea-Trade model of retinotopic development and this formed the rationale for the approach \cite{Durbin1987-ki, Willshaw1976-ew}. Competition was included in the same fashion as the Distributed Kernels model in the previous chapter: this resulted in an additional term in the energy functional. The functional was then parallelised on a GPU architecture and solved using a combination of the original graduated optimisation annealing method and the momentum accelerated gradient descent operator ADAM \cite{kingma2017adam}. There were two types of initialisation of the net explored: a small circle around a point in the search space, and an NMPeano fractal \cite{Moscato1994-nf}. There was also exploration of minimisation using meta-heuristics where an optimiser would attempt to find the best initialisation point in the search space. 

The algorithm performed well: with meta-heuristics it was able to achieve a 2\% error on relatively small tours while without meta-heuristic optimisation it converged to a consistent error of 7.5\% regardless of the size of the problem. This consistency was not observed in the original Elastic Net and was argued to arise from the competitive mechanism: the mechanism when acting in the absence of cities generates a space filling curve and attempts to place beads in the tour at equidistant locations. This space-filling nature motivated the initialisation by bootstrapping. The competitive mechanism makes tours with cross-overs energetically unfavourable and thus restricts the search space of possible solutions to those which are homotopically equivalent to the optimal. The best-in-class solvers are the LKH and EAX-GA algorithms and these outperform the Elastic Neighbourhood when they are given sufficient time. This is to be expected because they both will eventually search the entire solution space which the Elastic Neighbourhood wont do. When they were restricted to the time it takes for the Elastic Neighbourhood to converge they became uncompetitive particularly for large tours. This point was demonstrated by the solutions to the art tours which required hundreds of hours of supercomputer time to solve for EAX-GA while the Elastic Neighbourhood found solutions within 6\% error tolerance in under an hour.

The original Elastic Net demonstrated the ability to generate cortical feature maps but predicted that feature maps distort retinotopy while the effectiveness at solving the TSP provided a plausible explantation for the efficient wiring of neural systems that utilise these maps. These feature maps can also be be generated by the Elastic Neighbourhood with a more regular underlying retinotopic map. This regularity in conjunction with a more efficient and scalable solution to the TSP demonstrate that competition is an essential developmental mechanism to control regular and cost-effective solutions to brain development which do not require extensive genetic programming. 
\paragraph{Limitations}
The Elastic Neighbourhood predominately limited by solution quality and a restrictive code dependency. While it performs effectively in time constrained contexts there are few real world examples which require this constraint to be so tight. The code base currently relies on CUDA.jl as a dependency which, in turn, depends on the CUDA libraries provided by NVIDIA \cite{besard2018juliagpu, Nickolls2008-zz}. These dependencies are not critical scientifically as all libraries are open sourced and have been rigorously scrutinised. They do, however, necessitate that an end user use NVIDIA based hardware which can be both cost prohibitive and potential future point of failure if the company takes a different direction with its software packages.

\paragraph{Future Directions}
The model has shown to be an effective solver which converges, in the uniform case, to a relatively standard error. Given the speed at which the algorithm converges having a predictable error is a desirable quality for a heuristic; many tasks only require a solution within a given tolerance. Some small unrigorous pilot studies have shown that the error can be meaningfully changed by various factors: the optimisation scheme, the initialisation, the annealing schedule, and online manipulation of the hyper-parameters as the simulation progresses. These are all good candidates to pursue a lower error tolerance.

A second technical line of pursuit is compiler optimisation. The model is currently running on custom GPU kernels written with the CUDA.jl package \cite{ElasticNeighbourhood}. These are unlikely to be optimal implementations and optimising the code would likely improve performance runtime. Furthermore, the code utilises the CUDA packages provided by NVIDIA which restricts it to exclusively NVIDIA hardware. This limitation could be alleviated by porting the code base to a more abstract computational environment such as that provided by the GPUArrays.jl \cite{GPUArrays}. This would likely lead to less optimised code but would increase the algorithms openness and accessibility.
\section{Final Remarks}
Theoretical models of mouse retinotopy are transitioning from purely phenomenological descriptions of singular developmental processes into unified mechanistic models which make quantitative predictions. This has been largely due to a tandem effort where theoretical modelling and biological experimentation have isolated the mechanisms which explain a substantial portion of the variance has been attributed to several key mechanisms. With the increase in data quantitative assessment of models becomes a more pressing task and rigorous efforts to constrain and assess models has begun \cite{Hjorth2015-le}. A key component of that study was constraining the parameters used to assess the model between different genotypes. A valuable extension to this approach is to have a reference or benchmark dataset on which models can be fitted. There are currently a large number of optical imaging scanning datasets covering a range of mouse phenotypes and these can be unified and descriptive statistics for each mouse scan can be generated using the lattice method. This would allow for more rigorous regression methodologies to be applied to existing models which would constrain the parameters and offer key insights both into model performance but also to relative interactions between model mechanisms. This in turn would enhance theoretical exploratory power and allow for well-constrained falsifiable hypotheses to be generated and tested experimentally. The best way to achieve this is to incorporate existing optical imaging data into a single open-source database and encourage deposition of future experimental into this database; this would follow the example Eglen et. al (2014) \cite{Eglen2014-fo}. Developing this infrastructure as a skeleton to test modelling methodology would lead to more comprehensive understanding of models and their relation to biological data as well as more informative hypotheses for further experimentation and is therefore identified as a principal vector for research efforts in the field. 

A secondary broad line on which retinotopic models are developing is by incorporating time and making predictions about retinotopic development at critical time points. This is challenging because data has been typically restricted to developmental end-points or in the case of tracer injections single time points at which the animal must be killed. The best existing model relies on a procedure that does not have a reliable interpretation of time and can predict only developmental end-points; some recent research has examined interpreting the minimisation procedure as a developmental procedure which resulted in necessary modifications to the relative strengths of activity and chemical cues \cite{Lyngholm2019-fs}. Models with an emphasis on targeting key points developmental time courses will be valuable tools to guide experimental research in measuring and analysing these time courses. This would provide insight into developmental disorders and stages where intervention is critical, or provide avenues for such pathologies to be potentially reversed.